{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSIV77AMEB2iASBmdh6V+b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HustleDanie/ML-Classification/blob/main/TEMPLATE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYrkpjAxun7E"
      },
      "outputs": [],
      "source": [
        "# Improved code with hyperparameter tuning\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('your_data.csv')\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop('target_column', axis=1)\n",
        "y = df['target_column']\n",
        "\n",
        "# Impute missing values with the median\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
        "X = imputer.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'solver': ['lbfgs', 'liblinear', 'sag']\n",
        "}\n",
        "\n",
        "# Create the Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Perform grid search with cross-validation\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
        "grid_search.fit()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Improved code with hyperparameter tuning (continued)\n",
        "\n",
        "# Perform grid search with cross-validation\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Train the model with the best hyperparameters\n",
        "model_best = LogisticRegression(**best_params)\n",
        "model_best.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "y_pred = model_best.predict(X_test_scaled)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Test accuracy with best hyperparameters: {accuracy:.2f}')\n",
        "\n",
        "# Evaluate the model on the train set\n",
        "y_pred = model_best.predict(X_train_scaled)\n",
        "accuracy = accuracy_score(y_train, y_pred)\n",
        "print(f'Test accuracy with best hyperparameters: {accuracy:.2f}')\n",
        "\n",
        "\n",
        "# Visualize the coefficients\n",
        "plt.bar(X.columns, model_best.coef_[0])\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Coefficient')\n",
        "plt.title('Logistic Regression Coefficients')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Making a Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Visualizing the test result\n",
        "from matplotlib.colors import ListedColormap\n",
        "X_set, y_set = sc.inverse_transform(X_test), y_test\n",
        "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),\n",
        "                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\n",
        "plt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n",
        "             alpha = 0.75, cmap = ListedColormap(('salmon', 'dodgerblue')))\n",
        "plt.xlim(X1.min(), X1.max())\n",
        "plt.ylim(X2.min(), X2.max())\n",
        "for i, j in enumerate(np.unique(y_set)):\n",
        "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('salmon', 'dodgerblue'))(i), label = j)\n",
        "plt.title('Logistic Regression (Test set)')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Estimated Salary')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Visualizing the train set\n",
        "from matplotlib.colors import ListedColormap\n",
        "X_set, y_set = sc.inverse_transform(X_train), y_train\n",
        "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),\n",
        "                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\n",
        "plt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n",
        "             alpha = 0.75, cmap = ListedColormap(('salmon', 'dodgerblue')))\n",
        "plt.xlim(X1.min(), X1.max())\n",
        "plt.ylim(X2.min(), X2.max())\n",
        "for i, j in enumerate(np.unique(y_set)):\n",
        "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('salmon', 'dodgerblue'))(i), label = j)\n",
        "plt.title('Logistic Regression (Training set)')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Estimated Salary')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Making the predictive system\n",
        "\n",
        "input_data = (0.0307,0.0523,0.0653,0.0521,0.0611,0.0577,0.0665,0.0664,0.1460,0.2792,0.3877,0.4992,0.4981,0.4972,0.5607,0.7339,0.8230,0.9173,0.9975,0.9911,0.8240,0.6498,0.5980,0.4862,0.3150,0.1543,0.0989,0.0284,0.1008,0.2636,0.2694,0.2930,0.2925,0.3998,0.3660,0.3172,0.4609,0.4374,0.1820,0.3376,0.6202,0.4448,0.1863,0.1420,0.0589,0.0576,0.0672,0.0269,0.0245,0.0190,0.0063,0.0321,0.0189,0.0137,0.0277,0.0152,0.0052,0.0121,0.0124,0.0055)\n",
        "\n",
        "# changing the input_data to a numpy array\n",
        "input_data_as_numpy_array = np.asarray(input_data)\n",
        "\n",
        "# reshape the np array as we are predicting for one instance\n",
        "input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)\n",
        "\n",
        "prediction = model.predict(input_data_reshaped)\n",
        "print(prediction)\n",
        "\n",
        "if (prediction[0]==''):\n",
        "  print('')\n",
        "else:\n",
        "  print('')"
      ],
      "metadata": {
        "id": "-KQ4bhvPux2G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}